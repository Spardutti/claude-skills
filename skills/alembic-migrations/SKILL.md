---
name: alembic-migrations
description: "MUST USE when creating, editing, or reviewing Alembic migration scripts, env.py configuration, or SQLAlchemy model changes that require migrations. Enforces naming conventions, autogenerate review, data migration safety, downgrade correctness, and production deployment patterns."
---

# Alembic Migration Best Practices

Always define naming conventions, review every autogenerated migration, write complete downgrades, separate schema from data changes, and never reference application models in migration scripts.

## Naming Conventions — Define Once, Apply Everywhere

Without naming conventions, constraints get anonymous names that break downgrades and differ across databases.

```python
# BAD: no naming convention — constraints get random names
from sqlalchemy.orm import DeclarativeBase

class Base(DeclarativeBase):
    pass  # anonymous constraints: impossible to drop by name in downgrades

# GOOD: explicit naming convention on MetaData
from sqlalchemy import MetaData
from sqlalchemy.orm import DeclarativeBase

convention = {
    "ix": "ix_%(table_name)s_%(column_0_label)s",
    "uq": "uq_%(table_name)s_%(column_0_name)s",
    "ck": "ck_%(table_name)s_%(constraint_name)s",
    "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
    "pk": "pk_%(table_name)s",
}

class Base(DeclarativeBase):
    metadata = MetaData(naming_convention=convention)
```

This makes every index, unique constraint, foreign key, check constraint, and primary key predictable and portable.

## Migration File Naming — Use Timestamps

Default revision IDs are unordered hashes. Use epoch timestamps for chronological sorting in your filesystem.

```ini
# alembic.ini
file_template = %%(epoch)d_%%(rev)s_%%(slug)s
```

Results in: `1728372261_c0a05e0cd317_add_user_phone_column.py`

Always write descriptive `-m` messages:

```bash
# BAD
alembic revision --autogenerate -m "changes"

# GOOD
alembic revision --autogenerate -m "add_phone_column_to_users"
```

## Autogenerate — Always Review

Autogenerate is a starting point, not the final migration. Always review and adjust.

### What Autogenerate DETECTS Reliably

- Table additions and removals
- Column additions and removals
- Nullable status changes
- Basic index and explicitly-named unique constraint changes
- Basic foreign key constraint changes

### What Autogenerate CANNOT Detect

- Table renames (detected as drop + create)
- Column renames (detected as drop + add — **data loss!**)
- Anonymous constraints
- `Enum` type changes on non-supporting backends
- Standalone `CHECK`, `EXCLUDE`, `PRIMARY KEY` constraint changes
- Sequence additions/removals

### What Autogenerate Detects OPTIONALLY

```python
# env.py — enable these for full detection
context.configure(
    connection=connection,
    target_metadata=target_metadata,
    compare_type=True,            # detect column type changes (on by default)
    compare_server_default=True,  # detect server_default changes (OFF by default!)
)
```

### Fix Column Renames Manually

```python
# BAD: autogenerate output — drops column and creates new one (DATA LOSS)
def upgrade():
    op.drop_column("users", "name")
    op.add_column("users", sa.Column("full_name", sa.String(100)))

# GOOD: manually correct to a rename
def upgrade():
    op.alter_column("users", "name", new_column_name="full_name")

def downgrade():
    op.alter_column("users", "full_name", new_column_name="name")
```

### Fix Table Renames Manually

```python
# BAD: autogenerate output — drops table and creates new one (DATA LOSS)
def upgrade():
    op.drop_table("account")
    op.create_table("user_account", ...)

# GOOD: manually correct to a rename
def upgrade():
    op.rename_table("account", "user_account")

def downgrade():
    op.rename_table("user_account", "account")
```

## env.py Configuration

### Full Production-Ready env.py

```python
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context

from myapp.database import Base  # your DeclarativeBase with naming convention

config = context.config
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

target_metadata = Base.metadata


def include_object(object, name, type_, reflected, compare_to):
    """Exclude non-application tables from autogenerate."""
    if type_ == "table" and name.startswith("_"):
        return False
    return True


def run_migrations_offline():
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,
        compare_server_default=True,
        include_object=include_object,
    )
    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online():
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )
    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
            compare_server_default=True,
            include_object=include_object,
            render_as_batch=True,  # required for SQLite; safe to enable always
        )
        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

### Filter Tables with `include_object`

```python
# Skip specific tables (e.g., spatial_ref_sys from PostGIS)
def include_object(object, name, type_, reflected, compare_to):
    if type_ == "table" and name in {"spatial_ref_sys", "alembic_version"}:
        return False
    return True
```

## Complete Downgrades — Always

Every `upgrade()` must have a matching `downgrade()` that fully reverses it.

```python
# BAD: empty or missing downgrade
def downgrade():
    pass

# GOOD: complete reverse of upgrade
def upgrade():
    op.add_column("users", sa.Column("phone", sa.String(20), nullable=True))
    op.create_index("ix_users_phone", "users", ["phone"])

def downgrade():
    op.drop_index("ix_users_phone", table_name="users")
    op.drop_column("users", "phone")
```

### Downgrade Order Matters

Reverse operations in the **opposite order** of the upgrade. Drop indexes before dropping columns, drop foreign keys before dropping tables.

## Data Migrations — Never Reference Application Models

Models change over time. A migration referencing `User` model will break when the model changes later. Use inline table definitions or raw SQL.

```python
# BAD: imports application model — breaks when model changes
from myapp.models import User

def upgrade():
    users = User.query.all()
    for user in users:
        user.email = user.email.lower()

# GOOD (option 1): inline table definition — static, never changes
from sqlalchemy import table, column, String

def upgrade():
    user_table = table(
        "users",
        column("id", sa.Integer),
        column("email", String),
    )
    conn = op.get_bind()
    users = conn.execute(user_table.select().where(user_table.c.email.isnot(None)))
    for user in users:
        conn.execute(
            user_table.update()
            .where(user_table.c.id == user.id)
            .values(email=user.email.lower())
        )

# GOOD (option 2): raw SQL — simplest for straightforward changes
def upgrade():
    op.execute("UPDATE users SET email = LOWER(email) WHERE email IS NOT NULL")
```

### Batch Data Migrations for Large Tables

```python
# BAD: single UPDATE on millions of rows — locks table, may timeout
def upgrade():
    op.execute("UPDATE orders SET status = 'active' WHERE status IS NULL")

# GOOD: batch processing to avoid long locks
def upgrade():
    conn = op.get_bind()
    while True:
        result = conn.execute(
            sa.text(
                "UPDATE orders SET status = 'active' "
                "WHERE id IN ("
                "  SELECT id FROM orders WHERE status IS NULL LIMIT 1000"
                ")"
            )
        )
        if result.rowcount == 0:
            break
```

## Separate Schema and Data Migrations

Never mix DDL (schema changes) and DML (data changes) in the same migration. Some databases cannot roll back DDL inside a transaction.

```bash
# GOOD: two separate migrations
alembic revision --autogenerate -m "add_status_column_to_orders"
alembic revision -m "backfill_status_column_on_orders"
```

```python
# Migration 1: schema only
def upgrade():
    op.add_column("orders", sa.Column("status", sa.String(20), nullable=True))

def downgrade():
    op.drop_column("orders", "status")
```

```python
# Migration 2: data only
def upgrade():
    op.execute("UPDATE orders SET status = 'active' WHERE status IS NULL")

def downgrade():
    op.execute("UPDATE orders SET status = NULL")
```

## Common Operations Reference

### Add Non-Nullable Column to Existing Table

You cannot add a `NOT NULL` column without a default to a table with existing rows. Use a three-step approach:

```python
def upgrade():
    # 1. Add as nullable
    op.add_column("users", sa.Column("role", sa.String(20), nullable=True))
    # 2. Backfill
    op.execute("UPDATE users SET role = 'member' WHERE role IS NULL")
    # 3. Set NOT NULL
    op.alter_column("users", "role", nullable=False)

def downgrade():
    op.drop_column("users", "role")
```

### Create Index Concurrently (PostgreSQL)

For large tables, regular `CREATE INDEX` locks writes. Use concurrent index creation:

```python
from alembic import op

def upgrade():
    op.execute("CREATE INDEX CONCURRENTLY ix_orders_user_id ON orders (user_id)")

def downgrade():
    op.drop_index("ix_orders_user_id", table_name="orders")
```

Note: `CREATE INDEX CONCURRENTLY` cannot run inside a transaction. You may need `autocommit` mode for this migration.

### Batch Operations for SQLite

SQLite does not support most `ALTER TABLE` operations. Use `batch_alter_table`:

```python
def upgrade():
    with op.batch_alter_table("users") as batch_op:
        batch_op.add_column(sa.Column("phone", sa.String(20)))
        batch_op.alter_column("email", nullable=True)
        batch_op.drop_column("legacy_field")

def downgrade():
    with op.batch_alter_table("users") as batch_op:
        batch_op.add_column(sa.Column("legacy_field", sa.String(50)))
        batch_op.alter_column("email", nullable=False)
        batch_op.drop_column("phone")
```

Enable globally in `env.py` with `render_as_batch=True`.

## Testing Migrations

### Stairway Test — Upgrade and Downgrade Every Step

Test that every migration can upgrade and immediately downgrade cleanly:

```python
import pytest
from alembic.config import Config
from alembic import command
from alembic.script import ScriptDirectory

@pytest.fixture
def alembic_config():
    config = Config("alembic.ini")
    config.set_main_option("sqlalchemy.url", "sqlite:///test.db")
    return config

def test_stairway(alembic_config):
    """Test upgrade→downgrade for every revision."""
    script = ScriptDirectory.from_config(alembic_config)
    revisions = list(script.walk_revisions("base", "heads"))
    revisions.reverse()

    for revision in revisions:
        command.upgrade(alembic_config, revision.revision)
        command.downgrade(alembic_config, revision.down_revision or "base")
        command.upgrade(alembic_config, revision.revision)
```

### CI Check — Detect Pending Migrations

```bash
# Fails if models are out of sync with migrations
alembic check
```

Add this to CI to catch missing migrations before merge.

## Production Deployment

### Docker Compose — Migrations Before App Start

```yaml
app_migrations:
  image: myapp:latest
  command: ["alembic", "upgrade", "head"]
  depends_on:
    db:
      condition: service_healthy

app:
  image: myapp:latest
  depends_on:
    app_migrations:
      condition: service_completed_successfully
```

### Stamp Existing Database

When adopting Alembic on an existing database, stamp it at the initial revision without running migrations:

```bash
alembic stamp head
```

### Resolve Multiple Heads

When branches diverge (parallel development), merge them:

```bash
# Check for multiple heads
alembic heads

# Create merge migration
alembic merge -m "merge_heads" head1 head2
```

## Rules Summary

1. **Define naming conventions** on `MetaData` — predictable constraint names across all databases
2. **Use timestamp file templates** — chronological ordering in `alembic.ini`
3. **Review every autogenerated migration** — autogenerate misses renames, enums, standalone constraints
4. **Enable `compare_type` and `compare_server_default`** in `env.py`
5. **Fix renames manually** — autogenerate renders them as drop + create (data loss)
6. **Write complete downgrades** — reverse every operation in opposite order
7. **Never import application models** in migrations — use `table()/column()` or raw SQL
8. **Separate schema and data migrations** — never mix DDL and DML in one script
9. **Three-step non-nullable columns** — add nullable, backfill, then set NOT NULL
10. **Batch large data migrations** — avoid long-running transactions and table locks
11. **Use `batch_alter_table`** for SQLite — enable `render_as_batch=True` globally
12. **Run stairway tests** — upgrade/downgrade every revision in CI
13. **Run `alembic check` in CI** — fail builds when models are out of sync
14. **Create concurrent indexes** on large PostgreSQL tables — avoid write locks
15. **Use `alembic stamp head`** when adopting Alembic on an existing database
